---
title:  "[부스트캠프 Ai-tech] Week 2 학습정리: 8강"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - common_ustage
tags:
  - Boostcamp
last_modified_at: 2021-08-13
---

## 트랜스포머(Transformer)의 구조 

트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있습니다. 다만 다른 점은 인코더와 디코더라는 단위가 N개가 존재할 수 있다는 점입니다.

이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개를 사용하였습니다.  

![](/assets/images/transformer_2.png){: width="100%"}  

### 2. 어텐션(Attention)  

트랜스포머에는 총 3가지의 다른 어텐션이 사용됩니다.  

![](/assets/images/transformer_6.png){: width="100%"}  

`셀프 어텐션은 본질적으로 Query, Key, Value가 동일한 경우를 말합니다.` 반면, 세번째 그림 인코더-디코더 어텐션에서는 Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않습니다. 주의할 점은 여기서 `Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미`입니다.  

```
인코더의 셀프 어텐션 : Query = Key = Value
디코더의 마스크드 셀프 어텐션 : Query = Key = Value
디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터
```  

![](/assets/images/transformer_7.png){: width="100%"}  

위 그림은 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줍니다. 세 개의 어텐션에 추가적으로 '멀티 헤드'라는 이름이 붙어있습니다. 뒤에서 설명하겠지만, 이는 트랜스포머가 어텐션을 병렬적으로 수행하는 방법을 의미합니다.  


##### e) 멀티 헤드 어텐션(Multi-head Attention)  

이제 num_heads의 의미와 왜 차원을 축소시킨 벡터로 어텐션을 수행하였는지 이해해 보겠습니다.  

![](/assets/images/transformer_16.png){: width="100%"}  

트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단하였습니다. 그래서 d_(model)의 차원을 num_heads개로 나누어 
d_(model)/num_heads의 차원을 가지는 Q, K, V에 대해서 `num_heads개의 병렬 어텐션을 수행`합니다. 논문에서는 하이퍼파라미터인 num_heads의 값을 8로 지정하였고, 8개의 병렬 어텐션이 이루어지게 됩니다. 다시 말해 위에서 설명한 어텐션이 8개로 병렬로 이루어지게 되는데, 이때 각각의 어텐션 값 행렬을 어텐션 헤드라고 부릅니다. 이때 가중치 행렬 W_Q, W_K, W_V의 값은 8개의 어텐션 헤드마다 전부 다릅니다.  

병렬 어텐션으로 얻을 수 있는 효과는 무엇일까요? 그리스로마신화에는 머리가 여러 개인 괴물 히드라나 케로베로스가 나옵니다. 이 괴물들의 특징은 머리가 여러 개이기 때문에 여러 시점에서 상대방을 볼 수 있다는 겁니다. 이렇게 되면 시각에서 놓치는 게 별로 없을테니까 이런 괴물들에게 기습을 하는 것이 굉장히 힘이 들겁니다. 멀티 헤드 어텐션도 똑같습니다. `어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집하겠다는 겁니다.`

예를 들어보겠습니다. 앞서 사용한 예문 '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.'를 상기해봅시다. 단어 그것(it)이 쿼리였다고 해봅시다. 즉, it에 대한 Q벡터로부터 다른 단어와의 연관도를 구하였을 때 첫번째 어텐션 헤드는 '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면, 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있습니다. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다.  

병렬 어텐션을 모두 수행하였다면 모든 어텐션 헤드를 연결(concatenate)합니다. 모두 연결된 어텐션 헤드 행렬의 크기는 (seq_len, d_(model))이 됩니다. (결국엔 차원이 유지된다)  

어텐션 헤드를 모두 연결한 행렬은 또 다른 가중치 행렬 W_O를 곱하게 되는데, 이렇게 나온 결과 행렬이 멀티-헤드 어텐션의 최종 결과물입니다. 때 결과물인 멀티-헤드 어텐션 행렬은 인코더의 입력이었던 문장 행렬의 (seq_len, d_(model))크기와 동일합니다. 트랜스포머는 다수의 인코더를 쌓기 때문에 행렬의 크기는 계속 유지되어야 합니다.  

#### 2) 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)  

지금은 인코더를 설명하고 있지만, 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층입니다. 포지션-와이즈 FFNN는 쉽게 말하면 완전 연결 FFNN(Fully-connected FFNN)이라고 해석할 수 있습니다. 앞서 인공 신경망은 결국 벡터와 행렬 연산으로 표현될 수 있음을 배웠습니다. 아래는 포지션 와이즈 FFNN의 수식을 보여줍니다.  

![](/assets/images/transformer_18.png){: width="100%"}  

여기서 x는 앞서 멀티 헤드 어텐션의 결과로 나온 (seq_len, d_(model))의 크기를 가지는 행렬을 말합니다. 가중치 행렬 W_1은 (d_(model), d_ff)의 크기를 가지고, 가중치 행렬 W_2은 (d_ff, d_(model))의 크기를 가집니다. 논문에서 은닉층의 크기인 d_ff는 앞서 하이퍼파라미터를 정의할 때 언급했듯이 2,048의 크기를 가집니다.  

여기서 매개변수 W_1, b_1, W_2, b_2는 `하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용`됩니다. 하지만 `인코더 층마다는 다른 값`을 가집니다.  

### 4. 인코더에서 디코더(Decoder)로  

![](/assets/images/transformer_26.png){: width="100%"}  

지금까지 인코더에 대해서 정리해보았습니다. 이렇게 구현된 인코더는 총 num_layers만큼의 층 연산을 순차적으로 한 후에 `마지막 층의 인코더의 출력을` `디코더에게 전달`합니다. 인코더 연산이 끝났으므로 이제 디코더 연산이 시작되어 `디코더 또한 총 num_layers만큼의 연산`을 하는데, `이때 매번 인코더가 보낸 출력을` `각 디코더 층 연산에 사용`합니다. 이제 본격적으로 디코더에 대해서 이해해봅시다.

#### 1) 디코더의 첫번째 서브층 : 셀프 어텐션

![](/assets/images/transformer_27.png){: width="100%"}  

위 그림과 같이 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 트랜스포머 또한 seq2seq와 마찬가지로 Teacher Forcing을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장에 해당되는 \<sos> je suis étudiant의 문장 행렬을 한 번에 입력받습니다. 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다.  

여기서 문제가 있습니다. seq2seq의 디코더에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 받으므로 다음 단어 예측에 현재 시점 이전 이전에 입력된 단어들만 참고할 수 있습니다. 반면, 트랜스포머는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생합니다. 가령, suis를 예측해야 하는 시점이라고 해봅시다. seq2seq의 디코더라면 현재까지 디코더에 입력된 단어는 \<sos>와 je뿐일 것입니다. 반면, 트랜스포머는 이미 문장 행렬로 \<sos> je suis étudiant를 입력받았습니다.  

이를 위해 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다. 직역하면 '미리보기에 대한 마스크'라고 할 수 있습니다.  

![](/assets/images/transformer_28.png){: width="100%"}   

이제 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 다음과 같이 마스킹합니다.  

![](/assets/images/transformer_29.png){: width="100%"}   

트랜스포머에는 총 세 가지 어텐션이 존재하며, 각 어텐션 시 함수에 전달하는 마스킹은 다음과 같습니다.  

```
인코더의 셀프 어텐션: 패딩 마스크
디코더의 셀프 어텐션: 룩-어헤드 마스크
디코더의 인코더-디코더 어텐션: 패딩 마스크
```

#### 2) 디코더의 두번째 서브층 : 인코더-디코더 어텐션  

디코더의 두번째 서브층에 대해서 이해해봅시다. 인코더-디코더 어텐션은 Query가 디코더에서 만들어진 행렬인 반면, Key와 Value는 인코더에서 온 행렬입니다.  

디코더의 두번째 서브층을 확대해보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있습니다.  

![](/assets/images/transformer_30.png){: width="100%"}   

두 개의 화살표는 각각 Key와 Value를 의미하며, 이는 인코더의 마지막 층에서 온 행렬로부터 얻습니다. 반면, Query는 디코더의 첫번째 서브층의 결과 행렬로부터 얻는다는 점이 다릅니다. Query가 디코더 행렬, Key가 인코더 행렬일 때, 어텐션 스코어 행렬을 구하는 과정은 다음과 같습니다.  

![](/assets/images/transformer_31.png){: width="100%"}   

인코더와 마찬가지로 디코더도 num_layers개만큼 쌓아주면 트랜스포머 모델이 완성됩니다.  