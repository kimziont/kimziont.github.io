---
title:  "[NLP] 엘모(Embeddings from Language Model, ELMo)"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - word_embedding
tags:
  - NLP
last_modified_at: 2021-08-14
---

## Embeddings from Language Model

ELMo는 2018년에 제안된 새로운 워드 임베딩 방법론입니다. ELMo는 Embeddings from `Language Model`의 약자입니다. 해석하면 `언어 모델로 하는 임베딩`입니다. ELMo의 가장 큰 특징은 `사전 훈련된 언어 모델(Pre-trained language model)을 사용`한다는 점입니다.  

Bank라는 단어를 생각해봅시다. Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미를 가지는데, Word2Vec이나 GloVe 등으로 표현된 임베딩 벡터들은 이를 제대로 반영하지 못한다는 단점이 있습니다. 예를 들어서 Word2Vec이나 GloVe 등의 임베딩 방법론으로 Bank란 단어를 [0.2 0.8 -1.2]라는 임베딩 벡터로 임베딩하였다고 하면, 이 단어는 Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미임에도 불구하고 두 가지 상황 모두에서 [0.2 0.8 -1.2]의 벡터가 사용됩니다.

그렇다면 같은 표기의 단어라도 `문맥에 따라서 다르게 워드 임베딩`을 할 수 있으면 자연어 처리의 성능이 더 올라가지 않을까요? 단어를 임베딩하기 전에 전체 문장을 고려해서 임베딩을 하겠다는 것이죠. 그래서 탄생한 것이 문맥을 반영한 워드 임베딩(Contextualized Word Embedding)입니다.  

따라서 전체 문맥이 고려되어 단어가 임베딩 되기 위해서는 단방향 모델이 아닌 양방향 Language model이 필요합니다. 이러한 언어모델을 biLM(Bidirectional Language Model)이라고 합니다.  

![](/assets/images/elmo_0.png){: width="100%" height="70%"}  

이때 biLM의 입력이 되는 워드 임베딩 방법으로는 이 책에서는 다루지 않은 char CNN이라는 방법을 사용합니다. 이 임베딩 방법은 글자(character) 단위로 계산되는데, 이렇게 하면 마치 서브단어(subword)의 정보를 참고하는 것처럼 문맥과 상관없이 dog란 단어와 doggy란 단어의 연관성을 찾아낼 수 있습니다. 또한 이 방법은 OOV에도 견고한다는 장점이 있습니다.  

주의할 점은 앞서 RNN 챕터에서 설명한 양방향 RNN과 ELMo에서의 biLM은 다소 다릅니다. 양방향 RNN은 순방향 RNN의 은닉 상태와 역방향의 RNN의 은닉 상태를 다음 층의 입력으로 보내기 전에 연결(concatenate)시킵니다. biLM의 순방향 언어모델과 역방향 언어모델이 각각의 은닉 상태만을 다음 은닉층으로 보내며 훈련시킨 후에 ELMo 표현으로 사용하기 위해서 은닉 상태를 연결(concatenate)시키는 것과는 다릅니다.  

![](/assets/images/elmo_1.png){: width="100%" height="70%"}  

이 예제에서는 play란 단어가 임베딩이 되고 있다는 가정 하에 ELMo를 설명합니다. play라는 단어를 임베딩 하기위해서 ELMo는 위의 점선의 사각형 내부의 각 층의 결과값을 재료로 사용합니다. 다시 말해 해당 시점(time-step)의 BiLM의 각 층의 출력값을 가져옵니다. 그리고 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결(concatenate)하고 추가 작업을 진행합니다.  

여기서 각 층의 출력값이란 첫번째는 임베딩 층을 말하며, 나머지 층은 각 층의 은닉 상태를 말합니다. ELMo의 직관적인 아이디어는 각 층의 출력값이 가진 정보는 전부 서로 다른 종류의 정보를 갖고 있을 것이므로, 이들을 모두 활용한다는 점에 있습니다.  
 

### 1. ELMo가 임베딩 벡터를 얻는 과정  

#### 1) 각 층의 출력값을 연결(concatenate)한다  

![](/assets/images/elmo_2.png){: width="100%" height="70%"}  


#### 2) 각 층의 출력값 가중합한다  

![](/assets/images/elmo_3.png){: width="100%" height="70%"}  

#### 3) 벡터의 크기를 결정하는 스칼라 매개변수를 곱한다  

![](/assets/images/elmo_4.png){: width="100%" height="70%"}  

### 2. 최종적으로 얻게되는 임베딩 벡터

이렇게 `준비된(pretrained) ELMo 벡터`를 `Glove와 같은 방법으로 임베딩된 벡터와 연결(concatenate)`해 `새로운 임베딩 벡터`로 사용함으로써 `문맥에 따른 워드 임베딩 벡터`를 얻게 됩니다. 그리고 이 때, ELMo 표현을 만드는데 사용되는 사전 훈련된 언어 모델의 가중치는 고정시킵니다. 그리고 대신 위에서 사용한 s_1, s_2, s_3 와 감마는 훈련 과정에서 학습됩니다.  

![](/assets/images/elmo_5.png){: width="100%" height="70%"}  


### 3. 참조

[딥러닝을 이용한 자연어 처리 입문]](https://wikidocs.net/33930)  