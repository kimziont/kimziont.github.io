---
title:  "[NLP] 사전 훈련된 워드 임베딩(Pre-trained Word Embedding"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - word_embedding
tags:
  - NLP
last_modified_at: 2021-05-04
---

## 사전 훈련된 워드 임베딩(Pre-trained Word Embedding

위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 특히 훈련 데이터가 적은 경우 이를 기반으로 충분한 임베딩 벡터를 만들기 힘들어 더욱 유용합니다.  

임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩(원-핫)이 되어있어야 합니다.  

임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.  

정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다.  


### 사전 훈련된 GloVe 사용하기  

사전 훈련된 GloVe와 Word2Vec 임베딩을 사용해서 모델을 훈련시키는 실습을 진행해봅시다.  

```
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip
```

glove.6B.zip의 압축을 풀면 그 안에 4개의 파일이 있는데 여기서 사용할 파일은 glove.6B.100d.txt 파일입니다. 해당 파일은 하나의 줄당 101개의 값을 가지는 리스트를 갖고 있습니다. 한 개의 줄만 읽어보도록 하겠습니다.  

```
['the', '-0.038194', '-0.24487', '0.72812', ... 중략... '0.8278', '0.27062']
```

101개의 값 중에서 첫번째 값은 임베딩 벡터가 의미하는 단어를 의미하며, 두번째 값부터 마지막 값은 해당 단어의 임베딩 벡터의 100개의 차원에서의 각 값을 의미합니다.   