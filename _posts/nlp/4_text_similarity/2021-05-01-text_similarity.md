---
title:  "[NLP] 텍스트 유사도 이론편"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - text_similarity
tags:
  - NLP
last_modified_at: 2021-05-01
---

## 텍스트 유사도  
요즘 많이 보편화된 인공지능 스피커에게 다음과 같은 질문을 한다고 생각해봅시다.  

* 이 노래 누가 만들었어?  
* 지금 나오는 노래 작곡가가 누구야?  

위 두 문장에 대한 대답은 똑같을 것이지만 인공지능 스피커는 질문을 다르게 인식하기 때문에 각기 대답을 달아줘야 한다. 이는 데이터의 양이 증가하면 굉장한 비효율성을 야기하기 때문에 의미가 같은 문장들 간에는 같은 대답을 하도록 해야합니다. 이를 위해 우리는 문장간의 유사도를 측정할 수 있어야합니다.  

텍스트 유사도란 말 그대로 텍스트가 얼마나 유사한지를 표현하는 방식입니다. 이러한 유사도를 정량화하기 위해서는 어떤 방법을 사용하면 좋을까요? 우리는 앞에서 텍스트를 벡터화 하는 방법에 대해 공부했습니다. 그렇기 때문에 이 벡터간의 유사도를 텍스트 유사도의 척도로 사용하면 좋을 것 같습니다.  

벡터간의 유사도를 나타내는 방법에는 대표적으로 <span style="color:#fcf191;"><u>코사인 유사도, 유클리디언 유사도, 자카드 유사도</u></span>가 있습니다.  

### 코사인 유사도  
코사인 유사도는 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도를 의미합니다. 두 벡터의 방향이 완전히 동일한 경우에는 1의 값을 가지며, 90°의 각을 이루면 0, 180°로 반대의 방향을 가지면 -1의 값을 갖게 됩니다. 즉, 결국 코사인 유사도는 -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단할 수 있습니다. 코사인 유사도는 일반적으로 다른 유사도 측정 방법에 비해 성능이 좋은데 그 이유는 단순히 좌표상의 거리가 아닌 두 벡터간의 각도 개념이 더해졌기 때문입니다.  

![](/assets/images/cos_similarity.png){: width="80%" height="70%"}  

### 유클리디언 유사도  
유클리드 거리(euclidean distance)는 문서의 유사도를 구할 때 자카드 유사도나 코사인 유사도만큼, 유용한 방법은 아닙니다. 하지만 여러 가지 방법을 이해하고, 시도해보는 것 자체만으로 다른 개념들을 이해할 때 도움이 되므로 의미가 있습니다. 유클리디언 거리는 L2 거리라고도 불리며, n차원 공간에서 두 점 사이의 최단 거리를 구하는 접근법입니다.  

![](/assets/images/euclid_similarity.png){: width="80%" height="70%"}  

유클리디언 유사도는 단순히 두 점 사이의 거리를 뜻하기 때문에 값의 제한이 없습니다.  

### 자카드 유사도  
자카드(Jaccard) 유사도는 두 문장이 가지는 단어를 집합으로 만들어 두 집합의 유사도를 측정하는 방식입니다.  


![](/assets/images/jaccard_similarity.png){: width="80%" height="70%"}  

예시를 통해 살펴보면 다음과 같습니다.  
![](/assets/images/jaccard_similarity_ex.png){: width="80%" height="70%"}  

