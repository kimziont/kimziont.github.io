---
title:  "[NLP paper review] Sequence to Sequence Learning with Neural Networks"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_paper
tags:
  - NLP
last_modified_at: 2021-09-11
---   

## 1. 등장배경: Mapping sequences to sequences
데이터의 순서 정보가 중요한 task의 경우(예를 들어, 언어와 관련된 task), sequence간의 mapping이 가능해야합니다. 기존의 Deep neural network(DNN)는 labeling된 훈련 데이터 셋을 이용한 학습에서 훌륭한 성능을 보여줬습니다. 하지만 sequences to sequences와 관련한 task에서는 그렇지 못합니다. DNN은 입력과 타겟의 shape이 고정되어 있는 경우에 대해서만 적용이 가능합니다. 그 이유는 DNN은 보통 Linear layer를 통해 모델을 깊게 구성하는데 Linear layer가 가지고 있는 가중치 파라미터의 shape은 고정되어 있기 때문에 입력과 타겟의 shape이 바뀌면 안됩니다.  

`입력의 길이가 고정되어 있지 않은 경우`에 발생할 수 있는 문제를 해결해주는 모델이 바로 RNN계열의 모델입니다. RNN계열의 모델은 time step마다 들어오는 입력의 shape만 일정하게 해주면 `time step은 그때마다 다르게 할 수 있습니다.` 그저 time step만큼 모델을 recurrent해주기만 하면 됩니다. 이러한 장점 덕분에 `RNN계열의 모델은 sequence간의 mapping을 가능하게 해줍니다.` RNN계열의 모델에는 대표적으로 Vanilla RNN, LSTM, GRU가 있습니다. 

## 2. 사용한 기법: Long term dependency
sequence의 길이가 길어질수록 가장 먼저 등장했던 단어들에 대한 정보가 소실됩니다. 이 논문에서는 다음과 같은 문제를 두 가지 방법으로 해결하였습니다. 첫 번째는 `Long Short-Term Memory(LSTM)을 사용`한 것이고, 두 번째는 `입력 문장의 순서를 뒤집은 것`입니다.

## 3. 모델의 구조: Encoder and Decoder
단어의 수, 어순이 일치하는 언어라면 RNN계열 하나의 모델만을 이용해서 Mapping Word/Sequence to Word 방법으로 task를 해결할 수도 있지만, `단어의 수도 다르고, 어순도 다른 non-monotonic한 관계의 언어에서는` RNN계열 `모델 두 개(Encoder, Decoder)를 사용한 Sequence to Sequence모델`이 필요하다. 두 개의 모델을 사용하면 무시할 수 있는 수준의 계산 복잡도로 서로 다른 언어 쌍을 이용한 학습을 자연스럽게 만들어줍니다[1]  

### 1) Encoder  
Encoder에서는 Source sentence를 `고정된 차원의 벡터`로 만들어 Decoder에서 이를 매 time step마다 하나의 input으로 사용합니다. 이 벡터를 `context vector`라고 합니다.

### 2) Decoder
Decoder에서는 `먼저 hidden state를 구합니다.` 그리고 `token prediction합니다.` hidden state를 구할 때는, context vector와 이전의 hidden state, 이전의 predicted token을 input으로 이용합니다. 그렇게 hidden state를 구하면 hidden state와 context vector와 이전의 predicted token을 이용해 해당 time step에서의 token을 prediction합니다.

## 4. 세부사항

- 디코더에 \<SoS\>와 \<EoS\>를 사용하여 prediction을 시작하고 끝맺는다  
- Reversing Source sentences
- Beam search

## 5. 참조  

[1]: N.Kalchbrenner and P.Blunsom, Recurent continuous translation models, In EMNLP, 2013