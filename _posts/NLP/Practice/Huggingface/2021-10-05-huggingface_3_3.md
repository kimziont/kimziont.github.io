---
title:  "[NLP] Hugging face Chap3. A full training (customizing training)"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-05
---  

# Hugging face: A full training (customizing training)
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Fine-tuningë‹¨ê³„ì—ì„œì˜ `í•™ìŠµì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•`í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.ğŸ¤—  

## 1. Prepare for training

Now weâ€™ll see how to achieve the same results as we did in the last section without using the Trainer class. 

Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our tokenized_datasets, to take care of some things that the Trainer did for us automatically. Specifically, we need to:

(Trainerê°€ ì•Œì•„ì„œ í•´ì£¼ë˜ ê³¼ì •ì„ ì§ì ‘ ì†ë´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì •ì˜í•œ ì €í¬ì˜ tokenized_datasetsì„ ë‹¤ìŒì˜ ì‚¬í•­ë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤)  

- Remove the columns corresponding to values the model does not expect (like the __sentence1__ and __sentence2__ columns)
- Rename the column __label__ to __labels__ (because the model expects the argument to be named labels)
- Set the format of the datasets so they return PyTorch tensors instead of lists

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# tokenized_datasets ìˆ˜ì •
tokenized_datasets = tokenized_datasets.remove_columns(
    ["sentence1", "sentence2", "idx"]
)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

# We can then check that the result only has columns that our model will accept
tokenized_datasets["train"].column_names
--------------------------------------------------------------------------------------
['attention_mask', 'input_ids', 'labels', 'token_type_ids']
```

Now that this is done, we can easily define our `dataloaders`:  

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```  

To quickly check there is no mistake in the data processing, we can inspect a batch like this:  

```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
----------------------------------------------
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ëª¨ë¸ì„ ì •ì˜í•´ì„œ ì˜ ë™ì‘í•˜ëŠ”ì§€ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

outputs = model(**batch)

print(outputs.loss, outputs.logits.shape)
-------------------------------------------------------------------
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```  

ì´ì œ optimizer, lr_schedulerë§Œ ì •ì˜í•´ì£¼ë©´ ë©ë‹ˆë‹¤. ğŸ¤—`Transformers`ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  

```python
# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ defaultë¡œ ì‚¬ìš©í•˜ëŠ” optimizer, lr_schedulerì„ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤
from transformers import AdamW, get_scheduler

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader) # 1377

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)
```

## 2. Training loop  

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

## 3. Evaluation loop

```python
from datasets import load_metric

metric= load_metric("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
-------------------------------------------------
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ğŸ”” __multiple GPUs__  

The training loop we defined earlier works fine on a single CPU or GPU. But using the ğŸ¤—`Accelerate` library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like:  

```python
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

ì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.  

The first line to add is the import line. The second line instantiates an Accelerator object that will look at the environment and initialize the proper distributed setup. ğŸ¤—`Accelerate` handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use __accelerator.device__ instead of __device__).

Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to __accelerator.prepare__. This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the device (again, if you want to keep this you can just change it to use __accelerator.device__) and replacing loss.backward() with __accelerator.backward(loss)__.  

```
# TPUëŠ” ì „ì²´ ë°ì´í„°ì…‹ì´ ì§ì‚¬ê°í˜• shapeì¼ ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤ 
# -> (ë°°ì¹˜ë§ˆë‹¤ ì§ì‚¬ê°í˜• shape -> ì „ì²´ ë°ì´í„°ì…‹ ì§ì‚¬ê°í˜• shape)
âš ï¸ In order to benefit from the speed-up offered by Cloud TPUs, 
we recommend padding your samples to a fixed length with the 
`padding="max_length"` and `max_length` arguments of the tokenizer.
```  

<details>
<summary>hereâ€™s what the complete training loop looks like with ğŸ¤— Accelerate</summary>
<div markdown="1">       

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Putting this in a train.py script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command:  

```python
accelerate config
```
which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:
```python
accelerate launch train.py
```  

which will launch the distributed training.

If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a training_function and run a last cell with:  

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```
</div>
</details>  

