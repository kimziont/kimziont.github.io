---
title:  "[NLP] Hugging face Chap2. Putting it all together(powerful tokenizer API)"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-04
---  

# Hugging face: Powerful tokenizer API
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” tokenizerê°€ ì–¼ë§ˆë‚˜ high levelì˜ APIì¸ì§€ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.ğŸ¤—  

## 1. Multiple sentences  

Weâ€™ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.  

ğŸ¤—Transformers API can `handle all of this` for us with a high-level function that weâ€™ll dive into here. When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:  

(ëª¨ë¸ì— ë§ëŠ” tokenizerë¥¼ í˜¸ì¶œí•˜ë©´ tokenizerëŠ” raw textì— ëª¨ë¸ì— í•„ìš”í•œ ëª¨ë“  tokenizationê³¼ì •ì„ ì•Œì•„ì„œ í•´ì¤ë‹ˆë‹¤.)  

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Here, the model_inputs variable `contains everything thatâ€™s necessary for a model to operate well`. For DistilBERT, that includes the input IDs as well as the attention mask. Other models that accept additional inputs will also have those output by the tokenizer object.

- Handle __single__, __multiple__ sequence
- Support __padding__, __truncate__, __attention masking__ operation
- Support __diverse return type__
- Support __special tokens__

```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

```python
print(tokenizer.decode(model_inputs["input_ids"]))
--------------------------------------------------
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
```

ğŸ”” __ì–´ë–¤ parameterê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ?__  

ì´ ë¶€ë¶„ì€ íŒŒì´ì¬ ê´€ë ¨ëœ ë¶€ë¶„ì…ë‹ˆë‹¤. ê°œì¸ì ì¸ ê¶ê¸ˆì¦ìœ¼ë¡œ ì•Œì•„ë³´ë‹ˆ `inspect`ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `signature` í•¨ìˆ˜ê°€ ì´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ë°”ë¡œ ì˜ˆì‹œë¥¼ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  

```python
signature(tokenizer)
---------------------------------------
<Signature (text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding>
```

```python
# ì¢€ ë” ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ëŠ” ë°©ë²•
for param in signature(tokenizer).parameters.values():
    print(param)
-----------------------------------------------------------
text: Union[str, List[str], List[List[str]]]
text_pair: Union[str, List[str], List[List[str]], NoneType] = None
add_special_tokens: bool = True
padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False
truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False
max_length: Union[int, NoneType] = None
stride: int = 0
is_split_into_words: bool = False
pad_to_multiple_of: Union[int, NoneType] = None
return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None
return_token_type_ids: Union[bool, NoneType] = None
return_attention_mask: Union[bool, NoneType] = None
return_overflowing_tokens: bool = False
return_special_tokens_mask: bool = False
return_offsets_mapping: bool = False
return_length: bool = False
verbose: bool = True
**kwargs
```