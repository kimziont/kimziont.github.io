---
title:  "[NLP] Hugging face Chap2. Behind the pipeline"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-02
---  

# Hugging face: Behind the pipeline
HuggingfaceÏóê Í¥ÄÌïú Ìè¨Ïä§Ìä∏Îäî [Huggingface Í≥µÏãù ÌôàÌéòÏù¥ÏßÄ](https://huggingface.co/)Î•º Ï∞∏Í≥†ÌïòÏó¨ ÏûëÏÑ±ÌïòÏòÄÏúºÎ©∞ Í∑∏ Ï§ëÏóêÏÑúÎèÑ HuggingfaceÎ•º ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏóê Í¥ÄÌï¥ ÏπúÏ†àÌïòÍ≤å ÏÑ§Î™ÖÌï¥ ÎÜìÏùÄ Í∏Ä[(Huggingface course)](https://huggingface.co/course/chapter1)Ïù¥ ÏûàÏñ¥ Ïù¥Í≤ÉÏùÑ Î∞îÌÉïÏúºÎ°ú ÏûëÏÑ±ÌïòÏòÄÏäµÎãàÎã§.  

ÏïûÏóêÏÑú Î¥§Îçò pipeline APIÎäî Í∞ÑÎã®ÌïòÍ≥† ÎÇòÏùò taskÏóê Î∞îÎ°ú Ï†ÅÏö©Ìï† Ïàò ÏûàÏßÄÎßå, Î™®Îç∏ÏùÑ ÎÇòÏùò taskÏóê ÏïåÎßûÍ≤å Ï°∞Í∏à Îçî ÏàòÏ†ïÌïòÍ≥† Ïã∂Îã§Î©¥ ÎÇ¥Î∂ÄÎ•º Îì§Ïó¨Îã§ Î≥º ÌïÑÏöîÍ∞Ä ÏûàÏäµÎãàÎã§. Ïù¥Î≤à Ìè¨Ïä§Ìä∏ÏóêÏÑúÎäî pipeline API ÎÇ¥Î∂ÄÎ•º Ï°∞Í∏à Îçî ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥ÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§.ü§ó    

## 1. Introduction
Transformer Í∏∞Î∞òÏùò Î™®Îç∏Îì§ÏùÄ ÌÅ¨Í∏∞Í∞Ä ÍµâÏû•Ìûà ÌÅ¨Í≥†, ÌõàÎ†®, Î∞∞Ìè¨Í∞Ä ÏâΩÏßÄ ÏïäÏäµÎãàÎã§. Í≤åÎã§Í∞Ä ÏµúÍ∑ºÏóêÎäî TransformerÍ∏∞Î∞òÏùò ÏÉàÎ°úÏö¥ Î™®Îç∏Îì§Ïù¥ Í≥ÑÏÜç ÎÇòÏò§Í≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê, Í∑∏ ÎïåÎßàÎã§ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Î∞©Î≤ïÏùÑ ÏùµÌòÄÏïº ÌïúÎã§Î©¥ ÍµâÏû•Ìûà ÌûòÎì§ Í≤É ÏûÖÎãàÎã§. Îã§ÌñâÌûà HuggingfaceÎäî Ïù¥Îü¨Ìïú Ï†êÎì§ÏùÑ ÏóºÎëêÌï¥ÎëêÍ≥† LibraryÎ•º Í∞úÎ∞úÌñàÍ∏∞ ÎïåÎ¨∏ÌñàÏäµÎãàÎã§. Îã§ÏùåÏùÄ HuggingfaceÏóêÏÑú Ï†úÍ≥µÌïòÎäî LibraryÎì§Ïùò Ï≤†ÌïôÏûÖÎãàÎã§.  

- __Ease of use__: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.
- __Flexibility__: At their core, all models are simple PyTorch nn.Module or TensorFlow tf.keras.Model classes and can be handled like any other models in their respective machine learning (ML) frameworks.
- __Simplicity__: Hardly any abstractions are made across the library. The ‚ÄúAll in one file‚Äù is a core concept: a model‚Äôs forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

![](/assets/images/huggingface_8.png){: width="100%" height="70%"}  

## 2. Tokenizer  

Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to `convert the text inputs into numbers that the model can make sense of`.  

Tokenizing needs to be done in exactly the `same way as when the model was pretrained`, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it  

```python
from transformers import AutoTokenizer

# Ïö∞Î¶¨Í∞Ä ÏÇ¨Ïö©Ìï† Î™®Îç∏Ïùò Ïù¥Î¶Ñ
checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"

# checkpointÍ∞Ä ÌïôÏäµÌï† Îïå ÏÇ¨Ïö©ÎêòÏóàÎçò TokenizerÍ∞Ä Î∂àÎü¨ÏôÄÏßÑÎã§(downloaded and cached)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```  

Once we have the tokenizer, `we can directly pass our sentences to it` and `we‚Äôll get back a dictionary that‚Äôs ready to feed to our model!` The only thing left to do is to `convert the list of input IDs to tensors`.  

```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
]

# return_tensors -> if no type is passed, you will get a list of lists as a result
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
----------------------------------------------------------------------------------------
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

## 3. Going through the model

We can download our pretrained model in the same way we did with our tokenizer. ü§óTransformers provides an `AutoModel` class which also has a `from_pretrained` method:  

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```  

In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it.

```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
---------------------------------------------
torch.Size([2, 16, 768])
```  

Note that the outputs of ü§óTransformers models behave like `namedtuples` or `dictionaries`. You can access the elements by attributes (like we did) or by key (outputs["last_hidden_state"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).

- outputs.last_hidden_state
- outputs['last_hidden_state']
- outputs[0]  

There are many different architectures available in ü§óTransformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:  

- __*Model__ (retrieve the hidden states)
- __*ForCausalLM__
- __*ForMaskedLM__
- __*ForMultipleChoice__
- __*ForQuestionAnswering__
- __*ForSequenceClassification__
- __*ForTokenClassification__
- and others ü§ó  

For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the AutoModel class, but AutoModelForSequenceClassification:  

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

```python
# Ïó¨Í∏∞Îäî ÏúÑÏùò outputs.last_hidden_stateÍ∞Ä ÏïÑÎãàÎùº outputs.logitsÏù¥Íµ¨Îßå
print(outputs.logits.shape)
-----------------------------
torch.Size([2, 2])
```  

## 4. Postprocessing the output  

```python
print(outputs.logits)
---------------------------------
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

Those are `not probabilities but logits`, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they `need to go through a SoftMax layer` (all ü§óTransformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):  

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
------------------------------------------------------------------
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```  

To get the labels corresponding to each position, we can inspect the `id2label attribute` of the `model config` (more on this in the next section):  

```python
model.config.id2label
---------------------------
{0: 'NEGATIVE', 1: 'POSITIVE'}
```  

We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let‚Äôs take some time to dive deeper into each of those steps.