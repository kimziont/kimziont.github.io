---
title:  "[NLP] Hugging face ê³µë¶€í•˜ê¸° 5ì¼ì°¨: Models"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-04
---  

# Hugging face: Models
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.ğŸ¤—    

## 1. Creating a Model from scratch

In this section weâ€™ll take a closer look at creating and using a model. Weâ€™ll use the `AutoModel class`, which is handy when you want to instantiate any model from a checkpoint.

The AutoModel class and all of its relatives are actually `simple wrappers over the wide variety of models available in the library`. Itâ€™s a clever wrapper as it can `automatically guess the appropriate model architecture for your checkpoint`, and then instantiates a model with this architecture.

However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Letâ€™s take a look at how this works with a BERT model.

```python
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)

print(config)
--------------------------------------------
BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
```  

## 2. Loading Pretrained Model

The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in Chapter 1, this would `require a long time and a lot of data`, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, itâ€™s imperative to be able to share and reuse models that have already been trained.

Loading a Transformer model that is already trained is simple â€” we can do this using the `from_pretrained method`:  

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```  

As you saw earlier, we could replace BertModel with the equivalent `AutoModel class`. Weâ€™ll do this from now on as this produces `checkpoint-agnostic code`; if your code works for one checkpoint, it should work seamlessly with another.  

In the code sample above `we didnâ€™t use BertConfig`, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves  

(ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, configë¥¼ ì •ì˜í•  í•„ìš”ê°€ ì—†ë‹¤, í•™ìŠµì— ì‚¬ìš©ëœ configë¥¼ í™•ì¸í•´ë³´ê³  ì‹¶ë‹¤ë©´)  

```python
model.config
----------------------------------------------
DistilBertConfig {
  "_name_or_path": "distilbert-base-uncased-finetuned-sst-2-english",
  "activation": "gelu",
  "architectures": [
    "DistilBertForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "sst-2",
  "hidden_dim": 3072,
  "id2label": {
    "0": "NEGATIVE",
    "1": "POSITIVE"
  },
  "initializer_range": 0.02,
  "label2id": {
    "NEGATIVE": 0,
    "POSITIVE": 1
  },
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "transformers_version": "4.11.2",
  "vocab_size": 30522
}
```  

The weights have been downloaded and cached (so future calls to the from_pretrained method wonâ€™t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the __HF_HOME__ environment variable.  

## 3. Save Model

```python
model.save_pretrained("directory_on_my_computer")  
```

This saves two files to your disk:  

```
config.json

pytorch_model.bin
```  

If you take a look at the `config.json` file, `youâ€™ll recognize the attributes necessary to build the model architecture.` This file `also contains some metadata`, such as where the checkpoint originated and what ğŸ¤—Transformers version you were using when you last saved the checkpoint.  

The `pytorch_model.bin` file is known as the `state dictionary`; it contains all your `modelâ€™s weights`. The two files go hand in hand; the configuration is necessary to know your modelâ€™s architecture, while the model weights are your modelâ€™s parameters.