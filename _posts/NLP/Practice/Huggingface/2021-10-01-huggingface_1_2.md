---
title:  "[NLP] Hugging face Chap1. Transformers, what can they do?"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-01
---  

# Hugging face: Transformers, what can they do?
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆíŽ˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ìž‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìžˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ìž‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

HuggingfaceëŠ” ìžì—°ì–´ ì²˜ë¦¬(NLP)ë¥¼ ìœ„í•œ ìƒíƒœê³„(Ecosystem)ë¡œ ëŒ€í‘œì ìœ¼ë¡œ ðŸ¤—`Transformers`, ðŸ¤—`Datasets`, ðŸ¤—`Tokenizers`ê³¼ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.  

## 1. Transformers, what can they do?
ðŸ¤—Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê±°ë‚˜ ê³µìœ ëœ ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ í•´ì¤ë‹ˆë‹¤. [Model Hub](https://huggingface.co/models)ì—ëŠ” ìˆ˜ì²œê°œì˜ pretrained modelì„ ì œê³µí•˜ê³  ìžˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì›í•œë‹¤ë©´ ìžì‹ ì´ ë§Œë“  ëª¨ë¸ì„ Hubì— ê³µìœ í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.    

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ”, ðŸ¤—Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¤‘ìš”í•œ ë„êµ¬ì¤‘ í•˜ë‚˜ì¸ `pipeline API`ë¥¼ ì´ìš©í•´ ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ NLP taskë¥¼ í‘¸ëŠ” ì˜ˆì‹œë¥¼ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  

ðŸ”” __pipeline__   
> The most basic object in the ðŸ¤—Transformers library is the `pipeline`. It `connects a model with its necessary preprocessing and postprocessing steps`.  

> By default, this `pipeline selects a particular pretrained model that has been fine-tuned for specific task`. The model is downloaded and `cached when you create object`. If you rerun the command, the cached model will be used instead and `there is no need to download the model again`.

> There are three main steps involved when you pass some text to a pipeline:

- The text is preprocessed into a format the model can understand.
- The preprocessed inputs are passed to the model.
- The predictions of the model are post-processed, so you can make sense of them.



### 1) Mask Filling  

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
-------------------------------------------------------------------------
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

### 2) Question answering

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn"
)
--------------------------------------------------------
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

### 3) Sentiment analysis

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
----------------------------------------------------------------------
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```