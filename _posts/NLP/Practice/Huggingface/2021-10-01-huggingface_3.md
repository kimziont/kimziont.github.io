---
title:  "[NLP] Hugging face ê³µë¶€í•˜ê¸° 3ì¼ì°¨: How do Transformers work"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-01
---  

# Hugging face: How do Transformers work?
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” í˜„ì¬ NLP ê´€ë ¨ taskì—ì„œ ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì¡°ì¸ `Transformer`ì— ëŒ€í•´ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.ğŸ¤— 

## 1. A bit of Transformer history  

![](/assets/images/huggingface_4.png){: width="100%" height="70%"}  

Transformer êµ¬ì¡°ê°€ ì²˜ìŒ ì†Œê°œëœ ê²ƒì€ 2017ë…„ 6ì›”ì…ë‹ˆë‹¤. ì´ ë‹¹ì‹œ Transformer ì—°êµ¬ì˜ taskëŠ” ë²ˆì—­(translation)ì— ì§‘ì¤‘ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. Transformerê°€ ë„ì…ëœ ì´í›„ ë‹¤ì–‘í•œ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ëª¨ë¸ë“¤ì´ ë“±ì¥í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.  

- GPT: íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ëª¨ë¸ì„ ì´ìš©í•œ ì²« ë²ˆì§¸ pretrained ëª¨ë¸
- BERT: íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” ëª¨ë¸ì„ ì´ìš©í•œ pretrained ëª¨ë¸
- GPT-2: GPTë³´ë‹¤ ì„±ëŠ¥ì´ ë” í–¥ìƒëœ ëª¨ë¸
- DistilBERT: ëª¨ë¸ì˜ ë¬´ê²Œë¥¼ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì´ë©´ì„œ ê·¼ì‚¬í•œ BERTì™€ ê·¼ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸
- BART and T5: ì›ë˜ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°(Encoder-Decoder)ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•œ ëª¨ë¸
- GPT-3: Fine-tuningì—†ì´ë„(called zero-shot learning) ë‹¤ì–‘í•œ taskì— ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸


## 2. Language model

> All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained on `large amounts of raw text` in a self-supervised fashion. `Self-supervised learning` is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!

Before, a `causal language model` has been trained by predicting the next word in a sentence having read the n previous words. it means the output depends on the past and present inputs, but not the future ones.  

 Another example is `masked language model`, useful for specific practical tasks.  

 
![](/assets/images/huggingface_5.png){: width="100%" height="70%"}  

## 3. Transfer Learning
ìµœê·¼ì— ë‚˜ì˜¤ëŠ” ì–¸ì–´ ëª¨ë¸ë“¤(GPT, BERT, BART ë“±)ì€ ëª¨ë‘ Transfer Learningë°©ë²•ì„ ì´ìš©í•©ë‹ˆë‹¤.

- ì—„ì²­ë‚œ ì–‘ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ì§€ì‹(weights)ì€ specific taskì—ì„œ ì¶©ë¶„íˆ ì´ì ì´ ìˆë‹¤
- ì¸í”„ë¼ê°€ ì˜ êµ¬ì¶•ëœ ê¸°ì—…ë˜ëŠ” ë‹¨ì²´ì—ì„œ ì œê³µí•´ì£¼ë©´ ê°œì¸ì€ GPUí•˜ë‚˜ë¡œë„ ì¶©ë¶„íˆ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆë‹¤
- taskë§ˆë‹¤ êµ¬ì¡°ê°€ ë³µì¡í•´ì§€ì§€ ì•ŠëŠ”ë‹¤ 
 
![](/assets/images/huggingface_6.png){: width="100%" height="70%"}  

## 4. General Architecture  

![](/assets/images/huggingface_7.png){: width="100%" height="70%"}  

- __Encoder__: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire __understanding of the input__
- __Decoder__: The decoder uses the encoderâ€™s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for __generating outputs__

Each of these parts `can be used independently`, depending on the task:
- __Encoder-only models__: Good for tasks that __require understanding of the input, such as sentence classification and named entity recognition__  
- __Decoder-only models__: Good for __generative tasks such as text generation__
- __Encoder-decoder models__ : Good for __generative tasks that require an input, such as translation or summarization__
