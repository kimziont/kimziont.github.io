---
title:  "[NLP] Hugging face Chap3. Fine-tuning a pretrained model"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-05
---  

# Hugging face: Fine-tuning a pretrained model
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Pretrainedëœ ëª¨ë¸ì„ ë‚˜ì˜ taskì— ë§ê²Œ Fine-tuningí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.ğŸ¤—  

## 1. Processing data

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```  

Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.

In this section we will use as an example the __MRPC (Microsoft Research Paraphrase Corpus) dataset__, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Weâ€™ve selected it for this chapter because itâ€™s a small dataset, so itâ€™s easy to experiment with training on it.  

The Hub doesnâ€™t just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets here, and we recommend you try to load and process a new dataset once you have gone through this section (see the general documentation here). But for now, letâ€™s focus on the MRPC dataset! This is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.

The ğŸ¤—`Datasets` library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:  

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")

raw_datasets
----------------------------------------------------
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```  

This command downloads and caches the dataset, by default in ~/.cache/huggingface/dataset. Recall from Chapter 2 that you can customize your cache folder by setting the __HF_HOME__ environment variable.  

We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:

```python
raw_train_dataset = raw_datasets["train"]

raw_train_dataset[0]
----------------------------------------------------
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```  

We can inspect the features of our raw_train_dataset. This will tell us the type of each column:  

```python
raw_train_dataset.features
---------------------------------------------------------------------------------------------------
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

## 2. Dataset.map

Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the previous chapter, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in Chapter 2. So, one way to preprocess the training dataset is:  

```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```  

This works well, but it has the `disadvantage of returning a dictionary` (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also `only work if you have enough RAM to store your whole dataset during the tokenization` (whereas the datasets from the ğŸ¤—`Datasets` library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).

To keep the data as a dataset, we will use the [__Dataset.map__](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method. This also allows us some `extra flexibility`, if we need more preprocessing done than just tokenization. The map method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:  

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

tokenized_datasets
--------------------------------------------------------------------------------------
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```  

This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids. This will allow us to use the option __batched=True__ in our call to __map__, which will greatly speed up the tokenization. The tokenizer is backed by a tokenizer written in Rust from the ğŸ¤—Tokenizers library. This tokenizer can be very fast, but only if we give it lots of inputs at once.  

Weâ€™re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing.

Note that weâ€™ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: itâ€™s better to pad the samples when weâ€™re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!

(íŒ¨ë”© ì‘ì—…ì€ ì „ì²´ ë°ì´í„° ì…‹ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²ƒ ë³´ë‹¤, ê° ë°°ì¹˜ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤)  

The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as `dynamic padding`.  

To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ğŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:  

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

To test this new toy, letâ€™s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:  

```python
samples = tokenized_datasets["train"][:8]
samples = {
    k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]
}
[len(x) for x in samples["input_ids"]]
---------------------------------------------------------------------------------
[50, 59, 47, 67, 59, 50, 62, 32]
```

No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, `the maximum length inside the batch`. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Letâ€™s double-check that our data_collator is dynamically padding the batch properly:

```python
batch = data_collator(samples)

{k: v.shape for k, v in batch.items()}
----------------------------------------------
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```