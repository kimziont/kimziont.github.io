---
title:  "[NLP] Hugging face Chap2. Handling multiple sequences"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-04
---  

# Hugging face: Handling multiple sequences
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì—¬ëŸ¬ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ ìƒê¸±ëŠ” ë¬¸ì œì ê³¼ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.ğŸ¤—  

## 1. Multiple sentences  

In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:

- How do we handle multiple sequences?
- How do we handle multiple sequences of different lengths?
- Are vocabulary indices the only inputs that allow a model to work well?
- Is there such a thing as too long a sequence?  

Letâ€™s see what kinds of problems these questions pose, and how we can solve them using the ğŸ¤—Transformers API.    

## 2. Models expect a batch of inputs  

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
--------------------------------------------------------------------
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```  

Oh no! Why did this fail?  

The problem is that we sent a single sequence to the model, whereas ğŸ¤—Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence.  

(tokenizerê°€ í•˜ëŠ” ì¼ì„ tokenizer.tokenize()ì™€ tokenizer.convert_tokens_to_ids()ë¥¼ í†µí•´ ì„¸ë¶„í™”í•˜ì—¬ ëª¨ë¸ì— ë„£ì–´ë´¤ì§€ë§Œ ì˜¤ë¥˜ê°€ ë‚¬ìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ê¸°ë¥¼ ê¸°ëŒ€í•˜ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë§Œì•½ì— ì„¸ë¶„í™”í•´ì„œ ì§ì ‘ í•œë‹¤ë©´ ë§ˆì§€ë§‰ì— ì°¨ì›ì„ ì¶”ê°€í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. tokenizerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë°‘ì— ì˜ˆì‹œë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.)  

Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence, but if you look closely, youâ€™ll see that it didnâ€™t just convert the list of input IDs into a tensor, it `added a dimension on top of it`:  

```python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
---------------------------------------------------------------------------
# í•˜ë‚˜ì˜ ë¬¸ì¥ì´ ë“¤ì–´ì™”ì–´ë„ ìë™ìœ¼ë¡œ ì°¨ì›ì´ ì¶”ê°€ë˜ì–´ 2ì°¨ì›ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. -> ë¬¸ì¥ ìˆ˜ * í† í° ìˆ˜
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```



Batching is the act of sending multiple sentences through the model  

Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. Thereâ€™s a second issue, though. When youâ€™re trying to batch together two (or more) sentences, they might be of `different lengths`. If youâ€™ve ever worked with tensors before, you know that they need to be of `rectangular shape`. To work around this problem, we usually `pad` the inputs.  

## 3. Padding inputs

The following list of lists cannot be converted to a tensor:  

```python
batched_ids = [
  [200, 200, 200],
  [200, 200]
]
```

weâ€™ll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values.  

```python
padding_id = 100

batched_ids = [
  [200, 200, 200],
  [200, 200, padding_id]
]
```  

The padding token ID can be found in __tokenizer.pad_token_id__. Letâ€™s use it and send our two sentences through the model individually and batched together:

```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

# í•˜ë‚˜ì”© ë„£ì–´ë³´ì
sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]

# batchë¡œ ë¬¶ì–´ì„œ ë³´ë‚´ë³´ì
batched_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]

# ê° ê²°ê³¼ë¥¼ ë³´ì
print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
--------------------------------------------------------------------------
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)

tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)

tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)

# ë‘ ë²ˆì§¸ sentenceì˜ ê°’ì´ ë‹¤ë¥´ë‹¤ -> paddingì´ attention ì—°ì‚°ì— ì°¸ì—¬í–ˆê¸° ë•Œë¬¸ì—
# ê°™ê¸°ë¥¼ ì›í•œë‹¤
```

ì—¬ê¸°ì„œ, ë‘ ë²ˆì§¸ sentenceì˜ í…ì„œ ê°’ì´ ë‹¬ë¼ì¡ŒìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ batchë‹¨ìœ„ë¡œ ë„£ì„ ë•Œ paddingì„ í•´ì£¼ì—ˆê³  ì´ ê°’ì´ attention ê³„ì‚°ì— í•¨ê»˜ í¬í•¨ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê°™ì€ ë¬¸ì¥ì´ modelì—ì„œ ë‚˜ì˜¬ ë•Œ ë‹¤ë¥¸ ê°’ì´ ë‚˜ì˜¤ë©´ ì•ˆë˜ê¸° ë•Œë¬¸ì— ì €í¬ëŠ” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ paddingì€ attention ê³„ì‚°ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•  ê²ƒì…ë‹ˆë‹¤.  

`To get the same result` when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by `using an attention mask`.  

## 4. Attention masks  

Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to  

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
  [1, 1, 1],
  [1, 1, 0]
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
-----------------------------------------
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

## 5. Longer sequences

With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:

- Use a model with a longer supported sequence length.
- Truncate your sequences.  

Models have different supported sequence lengths, and some specialize in handling very long sequences. __Longformer__ is one example, and another is __LED__. If youâ€™re working on a task that requires very long sequences, we recommend you take a look at those models.

Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:  

```python
sequence = sequence[:max_sequence_length]
```