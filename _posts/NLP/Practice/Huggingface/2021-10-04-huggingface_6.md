---
title:  "[NLP] Hugging face ê³µë¶€í•˜ê¸° 6ì¼ì°¨: Tokenizers"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/NLP.jpg

categories:
  - nlp_huggingface
tags:
  - NLP
last_modified_at: 2021-10-04
---  

# Hugging face: Tokenizers
Huggingfaceì— ê´€í•œ í¬ìŠ¤íŠ¸ëŠ” [Huggingface ê³µì‹ í™ˆí˜ì´ì§€](https://huggingface.co/)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìœ¼ë©° ê·¸ ì¤‘ì—ì„œë„ Huggingfaceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ê´€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ë†“ì€ ê¸€[(Huggingface course)](https://huggingface.co/course/chapter1)ì´ ìˆì–´ ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Tokenizerì˜ ì¢…ë¥˜ì™€ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.ğŸ¤—  

## 1. Word-based  

In NLP tasks, the data that is generally processed is raw text. Hereâ€™s an example of such text:

```
Jim Henson was a puppeteer
```

However, models can only process numbers, so we need to find a way to `convert the raw text to numbers.` `Thatâ€™s what the tokenizers do`, and there are a lot of ways to go about this. The goal is to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation.

Letâ€™s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.  

The first type of tokenizer that comes to mind is `word-based`. Itâ€™s generally very easy to set up and use with only a few rules. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:  

![](/assets/images/huggingface_9.png){: width="100%" height="70%"}  

There are different ways to split the text. For example, we could could use whitespace to tokenize the text into words by applying Pythonâ€™s split function:  

```python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
---------------------------------------------------
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

If we want to completely cover a language with a word-based tokenizer, weâ€™ll need to have an identifier for each word in the language, which will `generate a huge amount of tokens`. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID weâ€™d need to keep track of that many IDs. Furthermore, words like `â€œdogâ€ are represented differently from words like â€œdogsâ€`, and the model will initially have no way of knowing that â€œdogâ€ and â€œdogsâ€ are similar: `it will identify the two words as unrelated.`  

Finally, we need a custom token to represent `words that are not in our vocabulary`. This is known as the â€œunknownâ€ token, often `represented as â€\[UNK]â€`. Itâ€™s generally a `bad sign` if you see that the tokenizer is producing a lot of these tokens, as `it wasnâ€™t able to retrieve a sensible representation` of a word and youâ€™re `losing information` along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.

One way to reduce the amount of unknown tokens is to go one level deeper, using a `character-based tokenizer`.  

## 2. Character-based  

Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:

- The vocabulary is much smaller.
- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.

![](/assets/images/huggingface_10.png){: width="100%" height="70%"}  

This approach isnâ€™t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, itâ€™s `less meaningful`: each character doesnâ€™t mean a lot on its own  

Another thing to consider is that weâ€™ll end up with a very `large amount of tokens to be processed by our model`: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.  

To get the best of both worlds, we can use a third technique that combines the two approaches: `subword tokenization`.  

## 3. Subword tokenization  
Subword tokenization algorithms rely on the principle that `frequently used words should not be split into smaller subwords`, but rare words should be decomposed into meaningful subwords.

For instance, â€œannoyinglyâ€ might be considered a rare word and could be decomposed into â€œannoyingâ€ and â€œlyâ€. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of â€œannoyinglyâ€ is kept by the composite meaning of â€œannoyingâ€ and â€œlyâ€.  

This approach is especially useful in agglutinative languages(êµì°©ì–´: êµì°©ì–´ë€ í•˜ë‚˜ì˜ ë‚±ë§(ì—„ë°€íˆëŠ” í•˜ë‚˜ì˜ ì–´ì ˆ)ì´ í•˜ë‚˜ì˜ ì–´ê·¼(root)ê³¼ ê°ê° ë‹¨ì¼í•œ ê¸°ëŠ¥ì„ ê°€ì§€ëŠ” í•˜ë‚˜ ì´ìƒì˜ ì ‘ì‚¬(affix)ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ” ì–¸ì–´ë¥¼ ë§í•œë‹¤. ì–´ê°„ê³¼ ì ‘ì‚¬ ì‚¬ì´ì˜ ê²½ê³„ë¥¼ ë¶„ëª…íˆ ì„¤ì •í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ì ‘ì‚¬ê°€ ëŒ€ì²´ë¡œ í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œì„ ê°€ì§€ê³  ìˆìœ¼ë©´ ê·¸ ì–¸ì–´ë¥¼ êµì°©ì–´ë¼ê³  í•œë‹¤.) such as Korean, where you can form (almost) arbitrarily long complex words by stringing together subwords.  

## 4. And more!  

- __Byte-level BPE__, as used in GPT-2
- __WordPiece__, as used in BERT
- __SentencePiece or Unigram__, as used in several multilingual models  

## 5. Loading and Saving

Loading and saving tokenizers is as simple as it is with models. Actually, itâ€™s based on the same two methods: `from_pretrained` and `save_pretrained`. These methods will load or save the `algorithm` used by the tokenizer as well as its `vocabulary`.

Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:  

```python
from transformers import BertTokenizer, AutoTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
# tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

```python
tokenizer("Using a Transformer network is simple")
-----------------------------------------------------------------
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

Weâ€™ll talk more about token_type_ids in Chapter 3, and weâ€™ll explain the attention_mask key a little later. First, letâ€™s see `how the input_ids are generated.` To do this, weâ€™ll need to look at the intermediate methods of the tokenizer.  

## 6. Encoding

Translating text to numbers is known as encoding. As weâ€™ve seen, the first step is to `split the text` into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to `make sure we use the same rules that were used when the model was pretrained.`

The second step is to `convert those tokens into numbers`, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a `vocabulary`, which is the part we download when we instantiate it with the __from_pretrained__ method. Again, we need to use the same vocabulary used when the model was pretrained.  

### 1) Tokenize

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
-----------------------------------------------------------
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

### 2) From tokens to input IDs

```python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
---------------------------------------------
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì¡°ê±´ì— ë§ì¶°ì„œ special tokenì„ ë‹¬ì•„ì£¼ëŠ” ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤.  

```python
ids = tokenizer.encode(sequence)

print(ids)
------------------------------------------------------
[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]

tokens = tokenizer.decode(ids)
------------------------------------------------------
[CLS] Using a Transformer network is simple [SEP]
```  

## 7. Decoding

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])

print(decoded_string)
------------------------------------------
'Using a Transformer network is simple'
```